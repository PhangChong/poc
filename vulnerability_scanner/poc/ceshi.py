# 存在robots.txt
import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning
import os
import urllib.parse
import urllib.request
import re
import time
import ssl
import urllib
from urllib.parse import urljoin, quote
import ServerJ


# bugName = '测试漏洞（robots.txt）'


def scan_ceshi(url, proxies, append_to_output, serverJ_key, bugName):
    path = "/robots.txt"
    if not url.startswith('http://') and not url.startswith('https://'):
        url = 'http://' + url

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/109.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Encoding": "gzip, deflate",
        "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
        "Connection": "close",
        "Upgrade-Insecure-Requests": "1"
    }

    target_url = url + path

    try:
        requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
        req = requests.get(target_url, headers=headers, verify=False, timeout=3, proxies=proxies)
        if req.status_code == 200 and 'U' in req.text:
            append_to_output(f"[+] {url} 存在{bugName}！！！！", "red")
            sc = ServerJ.sc_send(bugName, f"漏洞连接: {url}\r ", ServerJ_Key=serverJ_key)
            print("ceshi.py", sc)
        else:
            append_to_output(f"[-] {url} 不存在{bugName}", "green")
    except requests.Timeout:
        append_to_output(f"[!] 请求超时，跳过URL: {url}", "yellow")
    except requests.ConnectionError as conn_err:
        append_to_output(f"[!] 连接错误: {conn_err}", "yellow")
    except Exception as e:
        if 'HTTPSConnectionPool' in str(e) or 'Burp Suite Professional' in str(e):
            append_to_output(f"[-] {url} 证书校验错误或者证书被拒绝", "yellow")
        else:
            append_to_output(str(e), "yellow")
